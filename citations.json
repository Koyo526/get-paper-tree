{
  "offset": 0,
  "data": [
    {
      "citingPaper": {
        "paperId": "b3def070af1f3a28b87ab4e0539a762f0df90b4d",
        "url": "https://www.semanticscholar.org/paper/b3def070af1f3a28b87ab4e0539a762f0df90b4d",
        "title": "Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models",
        "abstract": "Denoising probabilistic diffusion models have shown breakthrough performance that can generate more photo-realistic images or human-level illustrations than the prior models such as GANs. This high image-generation capability has stimulated the creation of many downstream applications in various areas. However, we find that this technology is indeed a double-edged sword: We identify a new type of attack, called the Natural Denoising Diffusion (NDD) attack based on the finding that state-of-the-art deep neural network (DNN) models still hold their prediction even if we intentionally remove their robust features, which are essential to the human visual system (HVS), by text prompts. The NDD attack can generate low-cost, model-agnostic, and transferrable adversarial attacks by exploiting the natural attack capability in diffusion models. Motivated by the finding, we construct a large-scale dataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically evaluate the risk of the natural attack capability of diffusion models with state-of-the-art text-to-image diffusion models. We evaluate the natural attack capability by answering 6 research questions. Through a user study to confirm the validity of the NDD attack, we find that the NDD attack can achieve an 88% detection rate while being stealthy to 93% of human subjects. We also find that the non-robust features embedded by diffusion models contribute to the natural attack capability. To confirm the model-agnostic and transferrable attack capability, we perform the NDD attack against an AD vehicle and find that 73% of the physically printed attacks can be detected as a stop sign. We hope that our study and dataset can help our community to be aware of the risk of diffusion models and facilitate further research toward robust DNN models.",
        "year": 2023,
        "referenceCount": 53,
        "citationCount": 0,
        "influentialCitationCount": 0,
        "isOpenAccess": true,
        "fieldsOfStudy": [
          "Computer Science"
        ],
        "authors": [
          {
            "authorId": "145364206",
            "name": "Takami Sato"
          },
          {
            "authorId": "2235823008",
            "name": "Justin Yue"
          },
          {
            "authorId": "2236031372",
            "name": "Nanze Chen"
          },
          {
            "authorId": "2152169867",
            "name": "Ningfei Wang"
          },
          {
            "authorId": "2143841041",
            "name": "Qi Alfred Chen"
          }
        ]
      }
    },
    {
      "citingPaper": {
        "paperId": "2a89dbafb45e4f811114685a6327558bdb4d1141",
        "url": "https://www.semanticscholar.org/paper/2a89dbafb45e4f811114685a6327558bdb4d1141",
        "title": "Physical Adversarial Attack meets Computer Vision: A Decade Survey",
        "abstract": "Despite the impressive achievements of Deep Neural Networks (DNNs) in computer vision, their vulnerability to adversarial attacks remains a critical concern. Extensive research has demonstrated that incorporating sophisticated perturbations into input images can lead to a catastrophic degradation in DNNs' performance. This perplexing phenomenon not only exists in the digital space but also in the physical world. Consequently, it becomes imperative to evaluate the security of DNNs-based systems to ensure their safe deployment in real-world scenarios, particularly in security-sensitive applications. To facilitate a profound understanding of this topic, this paper presents a comprehensive overview of physical adversarial attacks. Firstly, we distill four general steps for launching physical adversarial attacks. Building upon this foundation, we uncover the pervasive role of artifacts carrying adversarial perturbations in the physical world. These artifacts influence each step. To denote them, we introduce a new term: adversarial medium. Then, we take the first step to systematically evaluate the performance of physical adversarial attacks, taking the adversarial medium as a first attempt. Our proposed evaluation metric, hiPAA, comprises six perspectives: Effectiveness, Stealthiness, Robustness, Practicability, Aesthetics, and Economics. We also provide comparative results across task categories, together with insightful observations and suggestions for future research directions.",
        "year": 2022,
        "referenceCount": 234,
        "citationCount": 15,
        "influentialCitationCount": 3,
        "isOpenAccess": true,
        "fieldsOfStudy": [
          "Computer Science"
        ],
        "authors": [
          {
            "authorId": "2186749480",
            "name": "Hui Wei"
          },
          {
            "authorId": "2112388769",
            "name": "Hao Tang"
          },
          {
            "authorId": "2100723008",
            "name": "Xuemei Jia"
          },
          {
            "authorId": "5731610",
            "name": "Han-Bing Yu"
          },
          {
            "authorId": "2186828480",
            "name": "Zhubo Li"
          },
          {
            "authorId": "2108315911",
            "name": "Zhixiang Wang"
          },
          {
            "authorId": "2128750265",
            "name": "Shinâ€™ichi Satoh"
          },
          {
            "authorId": "50219447",
            "name": "Zheng Wang"
          }
        ]
      }
    }
  ]
}